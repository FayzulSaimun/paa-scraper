{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjyTVhl4nn0R"
      },
      "source": [
        "importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsW6WulZngMz"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
        "import os\n",
        "from fake_useragent import UserAgent\n",
        "from typing import Dict\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "scrape_paa = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMGnVIGbn7b_"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D5kvdnsn9G-"
      },
      "outputs": [],
      "source": [
        "# latest user agent\n",
        "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36\"\n",
        "header = {\"User-Agent\": USER_AGENT}\n",
        "\n",
        "\n",
        "def get_link(s): #extract link\n",
        "    link = s.find(\n",
        "        lambda tag: (\n",
        "            tag.name == \"a\"\n",
        "            and tag.has_attr(\"href\")\n",
        "            and tag[\"href\"].startswith(\"http\")\n",
        "            and (tag.h3 or tag.h2) is not None\n",
        "        )\n",
        "    ).get(\"href\")\n",
        "    if \"#\" in link:\n",
        "        link = link.split(\"#\")[0]\n",
        "    return link\n",
        "\n",
        "\n",
        "def get_title(s): #extract title\n",
        "    try:\n",
        "      title = s.find(\n",
        "          lambda tag: (\n",
        "              tag.name == \"a\"\n",
        "              and tag.has_attr(\"href\")\n",
        "              and tag[\"href\"].startswith(\"http\")\n",
        "              and (tag.h3 or tag.h2) is not None\n",
        "          )\n",
        "      ).find_next('h3').text.strip()\n",
        "    except:\n",
        "        title = s.find(\n",
        "          lambda tag: (\n",
        "              tag.name == \"a\"\n",
        "              and tag.has_attr(\"href\")\n",
        "              and tag[\"href\"].startswith(\"http\")\n",
        "              and (tag.h3 or tag.h2) is not None\n",
        "          )\n",
        "      ).find_next('h2').text.strip()\n",
        "    return title\n",
        "\n",
        "\n",
        "def return_data(paa, seed, text, link, title): #return data as dict\n",
        "    return {\n",
        "        \"PAA Title\": paa,\n",
        "        \"Parent\": seed,\n",
        "        \"Text\": text,\n",
        "        \"URL\": link,\n",
        "        \"URL Title\": title,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_ul(s, paa, seed): #getting the ul element\n",
        "    text = (\n",
        "        s.find(\"div\", {\"role\": \"heading\", \"aria-level\": \"3\"})\n",
        "        .find_next(\"ul\")\n",
        "        .find_all(\"li\")\n",
        "    )\n",
        "    text = [a.text.strip() for a in text]\n",
        "    text = \"\".join(text)\n",
        "    link = get_link(s)\n",
        "    title = get_title(s)\n",
        "    data = return_data(paa, seed, text, link, title)\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_ol(s, paa, seed): #getting the ol element\n",
        "    text = (\n",
        "        s.find(\"div\", {\"role\": \"heading\", \"aria-level\": \"3\"})\n",
        "        .find_next(\"ol\")\n",
        "        .find_all(\"li\")\n",
        "    )\n",
        "    text = [a.text.strip() for a in text]\n",
        "    text = \"\".join(text)\n",
        "    link = get_link(s)\n",
        "    title = get_title(s)\n",
        "    data = return_data(paa, seed, text, link, title)\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_snippet(s, paa, seed): #getting normal snippet text\n",
        "    text = (\n",
        "        s.find(\"div\", {\"role\": \"heading\", \"aria-level\": \"3\"})\n",
        "        .find_next(\"span\", class_=\"hgKElc\")\n",
        "        .text.strip()\n",
        "    )\n",
        "    link = get_link(s)\n",
        "    title = get_title(s)\n",
        "    data = return_data(paa, seed, text, link, title)\n",
        "    return data\n",
        "\n",
        "\n",
        "def write_to_csv(df: Dict, filename, csv_header): #for writing to csv file\n",
        "    with open(f\"{filename}.csv\", \"a+\", newline=\"\", encoding = \"ISO-8859-1\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=csv_header)\n",
        "        f.seek(0, 2)\n",
        "        if f.tell() == 0:\n",
        "            writer.writeheader()\n",
        "        writer.writerow(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2GKQKnyn_jo"
      },
      "source": [
        "Getting Level 1 paa from seed keyword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgWTgylloG75"
      },
      "outputs": [],
      "source": [
        "session = requests.Session()\n",
        "def get_paa(seed, level, paa):\n",
        "    URL = \"https://www.google.com/search\"\n",
        "    params = {\"q\": seed, \"gl\": \"us\"}\n",
        "    while True:\n",
        "        response = session.get(URL, params=params, headers=header)\n",
        "        if response.status_code == 200:\n",
        "            break\n",
        "    s = bs(response.text, \"html5lib\")\n",
        "    # getting the paa questions\n",
        "    try:\n",
        "        div_questions = s.find_all(\"div\", class_=\"related-question-pair\")\n",
        "        get_text = lambda a: a.text.split(\"Search for:\")[0]\n",
        "        questions = list(map(get_text, div_questions))\n",
        "    except:\n",
        "        questions = []\n",
        "    if len(questions) > 0:\n",
        "        paa_file = f\"level_{level}\"\n",
        "        q_df = pd.DataFrame({\"paa\": questions})\n",
        "        q_df[\"parent\"] = seed\n",
        "        q_df_dict = q_df.to_dict(orient=\"records\")\n",
        "        csv_header = [\"paa\", \"parent\"]\n",
        "        for q in q_df_dict:\n",
        "            write_to_csv(q, paa_file, csv_header)\n",
        "seeds_keywords = [\"world cup\"]\n",
        "if scrape_paa:\n",
        "    for seed in seeds_keywords:\n",
        "        get_paa(seed, 1, seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orK7-IEToT1P"
      },
      "source": [
        "This function will collect the data from each paa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc1AMsXsK17c"
      },
      "outputs": [],
      "source": [
        "def get_paa_data(paa, level, seed): #paa: paa question, level: which level the paa is in, seed:parent paa/keyword for this paa\n",
        "    excluded_keywords = [\"health\", \"cancer\"]  #excluded keywords\n",
        "    if not any(keyword in paa for keyword in excluded_keywords):\n",
        "        csv_header = [\"PAA Title\", \"Parent\", \"Text\", \"URL\", \"URL Title\"]\n",
        "        URL = \"https://www.google.com/search\"\n",
        "        params = {\"q\": paa, \"gl\": \"us\"}\n",
        "        while True: #trying until success\n",
        "            r = session.get(URL, params=params, headers=header)\n",
        "            if r.status_code == 200:\n",
        "                break\n",
        "        sp = bs(r.text, \"html5lib\")\n",
        "        # ul snippet\n",
        "        try:\n",
        "            if (\n",
        "                sp.find(\"div\", {\"role\": \"heading\", \"aria-level\": \"3\"}).find_next(\"ul\")\n",
        "                is not None\n",
        "            ):\n",
        "                data = get_ul(sp, paa, seed)\n",
        "                write_to_csv(data, fileName, csv_header)\n",
        "                # print(data)\n",
        "            # ol list\n",
        "            if (\n",
        "                sp.find(\"div\", {\"role\": \"heading\", \"aria-level\": \"3\"}).find_next(\"ol\")\n",
        "                is not None\n",
        "            ):\n",
        "                data = get_ol(sp, paa, seed)\n",
        "                write_to_csv(data, fileName, csv_header)\n",
        "                # print(data)\n",
        "            # ul list\n",
        "            if (\n",
        "                sp.find(\"div\", {\"role\": \"heading\", \"aria-level\": \"3\"}).find_next(\n",
        "                    \"span\", class_=\"hgKElc\"\n",
        "                )\n",
        "                is not None\n",
        "            ):\n",
        "                data = get_snippet(sp, paa, seed)\n",
        "                write_to_csv(data, fileName, csv_header)\n",
        "                # print(data)\n",
        "        except:\n",
        "            # print(\"Nothing here\")\n",
        "            pass\n",
        "        try:\n",
        "            div_questions = sp.find_all(\"div\", class_=\"related-question-pair\")\n",
        "            get_text = lambda a: a.text.split(\"Search for:\")[0]\n",
        "            questions = list(map(get_text, div_questions))\n",
        "        except:\n",
        "            questions = []\n",
        "        if len(questions) > 0:\n",
        "            paa_file = f\"level_{level}\"\n",
        "            q_df = pd.DataFrame({\"paa\": questions})\n",
        "            q_df[\"parent\"] = paa\n",
        "            q_df_dict = q_df.to_dict(orient=\"records\")\n",
        "            csv_header = [\"paa\", \"parent\"]\n",
        "            for q in q_df_dict:\n",
        "                write_to_csv(q, paa_file, csv_header)\n",
        "paa_level = 4 #change your level here\n",
        "if scrape_paa:\n",
        "    fileName = f\"google_ppa_{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
        "    for i in range(1, paa_level + 1): #you can change the level here.\n",
        "        level = pd.read_csv(f\"level_{i}.csv\") #reading the level csv file\n",
        "        level = level.drop_duplicates(subset=[\"paa\"], keep=\"first\") #removing duplicates\n",
        "        level_dict = level.to_dict(orient=\"records\")\n",
        "        items = [\n",
        "            {\"paa\": level_dict[p][\"paa\"], \"level\": i + 1, \"seed\": level_dict[p][\"parent\"]}\n",
        "            for p in range(len(level_dict))\n",
        "        ]  #items list for threadpool\n",
        "        def run(my_iter):\n",
        "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "                results = list(tqdm(executor.map(lambda f: get_paa_data(**f), my_iter), total = len(my_iter)))\n",
        "        run(items)\n",
        "    files_to_delete = [\n",
        "        \"level_1.csv\",\n",
        "        \"level_2.csv\",\n",
        "        \"level_3.csv\",\n",
        "        \"level_4.csv\",\n",
        "        \"level_5.csv\",\n",
        "        \"level_6.csv\",\n",
        "        \"level_7.csv\",\n",
        "        \"level_8.csv\",\n",
        "        \"level_9.csv\"\n",
        "    ] #removing these files because in each run they will generate.\n",
        "    for f in files_to_delete:\n",
        "        if os.path.exists(f):\n",
        "            os.remove(f)\n",
        "            print(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next section is for keyword clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import cluster\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "sw = stopwords.words('english')\n",
        "def tokenizer(keyword):\n",
        "    return [stemmer.stem(w) for w in keyword.split()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read the latest file from directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "list_of_files = glob.glob('*.csv') # * means all if need specific format then *.csv\n",
        "latest_file = max(list_of_files, key=os.path.getctime)\n",
        "print(latest_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keywords_df = pd.read_csv(latest_file, encoding= \"ISO-8859-1\") #change the fileName here\n",
        "keywords_df = keywords_df.drop_duplicates(subset=['PAA Title'],keep='first')\n",
        "keywords = keywords_df['PAA Title'].values.tolist()\n",
        "keywords = [k.replace(\"?\", \"\").strip() for k in keywords]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(tokenizer=tokenizer, stop_words=sw)\n",
        "X = pd.DataFrame(tfidf.fit_transform(keywords).toarray(),\n",
        "                 index=keywords, columns=tfidf.get_feature_names_out())\n",
        "c = cluster.AffinityPropagation() # For prediction\n",
        "X['pred'] = c.fit_predict(X) #adding to dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = X.reset_index()\n",
        "X = X[['index','pred']]\n",
        "X = X.rename(columns={'index':\"paa_title\"})\n",
        "keywords_df['paa_title'] = X['paa_title']\n",
        "keywords_df['clusters'] = X['pred']\n",
        "keywords_df.to_csv(f'{fileName}_clustered_file.csv', index=False, encoding='ISO-8859-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PAA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
